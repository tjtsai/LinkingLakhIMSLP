{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path\n",
    "import glob\n",
    "import pickle\n",
    "import time\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate bootleg scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadPickle(pkl_file):\n",
    "    with open(pkl_file, 'rb') as f:\n",
    "        d = pickle.load(f)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def savePickle(pkl_file, d):\n",
    "    with open(pkl_file, 'wb') as f:\n",
    "        pickle.dump(d, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregateBootlegScores(score_feat_dir, outdir):\n",
    "    if not os.path.isdir(outdir):\n",
    "        os.makedirs(outdir)\n",
    "    for curdir in glob.glob('{}/*'.format(score_feat_dir)):\n",
    "        \n",
    "        # concatenate bootleg score fragments\n",
    "        pieceid = os.path.basename(curdir)\n",
    "        outfile = '{}/{}.pkl'.format(outdir, pieceid)\n",
    "        if os.path.exists(outfile):\n",
    "            continue\n",
    "        numFiles = len(glob.glob('{}/*.pkl'.format(curdir)))\n",
    "        fragments = []\n",
    "        for i in range(numFiles):\n",
    "            pkl_file = '{}/{}-{}.pkl'.format(curdir, pieceid, i)\n",
    "            d = loadPickle(pkl_file)\n",
    "            assert 'bscore' in d, \"bscore key not found in {}\".format(pkl_file)\n",
    "            bscore = d['bscore']\n",
    "            if bscore is not None and isinstance(bscore, np.ndarray) and bscore.ndim == 2 and bscore.shape[0] == 62:\n",
    "                fragments.append(bscore)\n",
    "        \n",
    "        # save global bootleg score\n",
    "        if len(fragments) == 0:\n",
    "            savePickle(outfile, None)\n",
    "        else:\n",
    "            concat = np.hstack(fragments)\n",
    "            savePickle(outfile, concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_feat_dir = 'score_feat' # directory containing sheet music bootleg scores\n",
    "score_feat_agg_dir = 'score_feat_agg' # directory containing aggregated bootleg scores\n",
    "aggregateBootlegScores(score_feat_dir, score_feat_agg_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Reverse Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constructSingleIndex(score_feat_dir, savefile = None):\n",
    "    num_files_p = len(glob.glob('{}/p*.pkl'.format(score_feat_dir)))\n",
    "    num_files_f = len(glob.glob('{}/f*.pkl'.format(score_feat_dir)))\n",
    "    num_files_total = num_files_p + num_files_f\n",
    "    d = {} # key: 64-bit fp value, value: list of (pieceNum, offset) tuples\n",
    "    for pieceNum in range(1, num_files_total + 1):\n",
    "        if pieceNum < num_files_p + 1:\n",
    "            curfile = '{}/p{}.pkl'.format(score_feat_dir, pieceNum)\n",
    "        else:\n",
    "            curfile = '{}/f{}.pkl'.format(score_feat_dir, pieceNum - num_files_p)\n",
    "        if not os.path.exists(curfile):\n",
    "            assert False, \"Cannot find {}\".format(curfile)\n",
    "        fps = extractFingerprints_sheet(curfile)\n",
    "        if len(fps) == 0:\n",
    "            continue\n",
    "        for i, fp in enumerate(fps):\n",
    "            if fp not in d:\n",
    "                d[fp] = []\n",
    "            if i < len(fps) - 2:\n",
    "                d[fp].append((pieceNum, i, fps[i+1], fps[i+2]))\n",
    "            else:\n",
    "                d[fp].append((pieceNum, i, 0, 0))\n",
    "\n",
    "    if savefile:\n",
    "        with open(savefile, 'wb') as f:\n",
    "            pickle.dump([d, num_files_total], f)\n",
    "    return d, num_files_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFingerprints_sheet(pkl_file):\n",
    "    '''\n",
    "    Convert binary matrix to list of 64-bit int fingerprints.\n",
    "    '''\n",
    "    D = loadPickle(pkl_file) # 62 x N binary matrix\n",
    "    if D is None:\n",
    "        return []\n",
    "    numBits = D.shape[0]\n",
    "    if D.shape[1] <= 1: # empty or contains single filler column\n",
    "        return []\n",
    "    assert numBits < 64, \"Number of bits must be less than 64.\"\n",
    "    mirrored = mirrorBothHands(D)\n",
    "    mask = np.power(2, np.arange(numBits)).reshape((1,-1))\n",
    "    fps = np.squeeze(mask @ mirrored)\n",
    "    return fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mirrorBothHands(bscore):\n",
    "    mirrored = np.maximum(bscore[18:28,:], bscore[28:38,:]) # overlap between hands is E3 to G4 inclusive\n",
    "    bscore[18:28,:] = mirrored\n",
    "    bscore[28:38,:] = mirrored\n",
    "    return bscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadSingleIndexDB(pkl_file):\n",
    "    d = loadPickle(pkl_file)\n",
    "    dsingle = d[0]\n",
    "    numFiles = d[1]\n",
    "    return dsingle, numFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "singleIndexFile = 'dbSingle.pkl'\n",
    "constructSingleIndex(score_feat_agg_dir, singleIndexFile)\n",
    "dsingle, dbSize = loadSingleIndexDB(singleIndexFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constructTripleIndex(d_singles, thresh):\n",
    "    d_triples = {}\n",
    "    for fp in d_singles:\n",
    "        listTups = d_singles[fp]\n",
    "        if len(listTups) >= thresh:\n",
    "            for (pieceNum, offset, fpA, fpB) in listTups:\n",
    "                fpNew = fp + fpA // 2 + fpB // 4\n",
    "                if fpNew not in d_triples:\n",
    "                    d_triples[fpNew] = []\n",
    "                d_triples[fpNew].append((pieceNum, offset))\n",
    "    return d_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveFullIndexDB(pkl_file, dbsingle, dbtriple, dbsize, dbthresh):\n",
    "    with open(pkl_file, 'wb') as f:\n",
    "        pickle.dump({'dbsingle': dbsingle, 'dbtriple': dbtriple, 'dbsize': dbsize, 'dbthresh': dbthresh}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFullIndexDB(pkl_file):\n",
    "    with open(pkl_file, 'rb') as f:\n",
    "        d = pickle.load(f)\n",
    "    dbsingle = d['dbsingle']\n",
    "    dbtriple = d['dbtriple']\n",
    "    dbsize = d['dbsize']\n",
    "    dbthresh = d['dbthresh']\n",
    "    return (dbsingle, dbtriple, dbsize, dbthresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbThresh = 8000\n",
    "dtriple = constructTripleIndex(dsingle, dbThresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullIndexFile = 'dbAll.pkl'\n",
    "saveFullIndexDB(fullIndexFile, dsingle, dtriple, dbSize, dbThresh)\n",
    "dbInfo = loadFullIndexDB(fullIndexFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFingerprintStats(d):\n",
    "    \n",
    "    # compute frequency of fingerprint occurrences\n",
    "    freqs = []\n",
    "    for key in d:\n",
    "        freqs.append(len(d[key]))\n",
    "    freqs = np.array(freqs)\n",
    "    freqs_sorted = sorted(freqs)\n",
    "    \n",
    "    # print useful stats\n",
    "    print('Number of unique fingerprints: {}'.format(len(freqs)))\n",
    "    print('Number of fingerprints that occur exactly once: {}'.format(np.sum(freqs == 1)))\n",
    "    print('5 most frequently occuring fingerprints occur _ times: {}'.format(freqs_sorted[::-1][0:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getFingerprintStats(dbInfo[0]) # single fingerprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getFingerprintStats(dbInfo[1]) # triple fingerprints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processMidiQuery(midi_bscore_file, dbInfo, sampleDur, nsamples, binSize, savefile = None):\n",
    "    \n",
    "    # load DB, start profiling\n",
    "    #print('Processing {}'.format(midi_bscore_file))\n",
    "    profileStart = time.time()\n",
    "    \n",
    "    # generate random samples from sharp & flat bscores\n",
    "    d = loadPickle(midi_bscore_file)\n",
    "    bscoreSharp = d['bscoreSharp']\n",
    "    bscoreFlat = d['bscoreFlat']\n",
    "    bscoreLength = bscoreSharp.shape[1]\n",
    "    sorted_sharp = []\n",
    "    sorted_flat = []\n",
    "    scores_sharp = []\n",
    "    scores_flat = []\n",
    "    np.random.seed(0)\n",
    "    for i in range(nsamples):\n",
    "        if sampleDur == -1: # use entire midi bscore\n",
    "            bscoreSharp_sample = bscoreSharp\n",
    "            bscoreFlat_sample = bscoreFlat\n",
    "        else: # use randomly selected sample\n",
    "            offset = np.random.randint(max(bscoreLength - sampleDur + 1, 1))\n",
    "            bscoreSharp_sample = bscoreSharp[:,offset:offset+sampleDur]\n",
    "            bscoreFlat_sample = bscoreFlat[:,offset:offset+sampleDur]    \n",
    "        sorted_pids_sharp, scores_pids_sharp = processMidiBscore(bscoreSharp_sample, dbInfo, binSize)\n",
    "        sorted_pids_flat, scores_pids_flat = processMidiBscore(bscoreFlat_sample, dbInfo, binSize)\n",
    "        sorted_sharp.append(sorted_pids_sharp)\n",
    "        scores_sharp.append(scores_pids_sharp)\n",
    "        sorted_flat.append(sorted_pids_flat)\n",
    "        scores_flat.append(scores_pids_flat)\n",
    "        \n",
    "    # select the one with the higher score\n",
    "    sorted_both = []\n",
    "    for i in range(nsamples):\n",
    "        score_sharp = np.max(scores_sharp[i])\n",
    "        score_flat  = np.max(scores_flat[i])\n",
    "        if score_sharp > score_flat:\n",
    "            sorted_both.append(sorted_sharp[i])\n",
    "        else:\n",
    "            sorted_both.append(sorted_flat[i])\n",
    "            \n",
    "    profileEnd = time.time()\n",
    "    profileDur = profileEnd - profileStart\n",
    "    \n",
    "    # save results\n",
    "    sorted_both = np.array(sorted_both)\n",
    "    sorted_sharp = np.array(sorted_sharp)\n",
    "    sorted_flat = np.array(sorted_flat)\n",
    "    scores_sharp = np.array(scores_sharp)\n",
    "    scores_flat = np.array(scores_flat)\n",
    "    results = {}\n",
    "    results['sorted_both'] = sorted_both\n",
    "    results['sorted_sharp'] = sorted_sharp\n",
    "    results['sorted_flat'] = sorted_flat\n",
    "    results['scores_sharp'] = scores_sharp\n",
    "    results['scores_flat'] = scores_flat\n",
    "    results['query'] = midi_bscore_file\n",
    "    results['dbSize'] = dbInfo[2]\n",
    "    results['dbThresh'] = dbInfo[3]\n",
    "    results['sampleDur'] = sampleDur\n",
    "    results['nsamples'] = nsamples\n",
    "    results['binSize'] = binSize\n",
    "    results['profileDur'] = profileDur\n",
    "    \n",
    "    if savefile:\n",
    "        with open(savefile, 'wb') as f:\n",
    "            pickle.dump(results, f)\n",
    "    \n",
    "    return sorted_both, sorted_sharp, sorted_flat, scores_sharp, scores_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processMidiBscore(bscore, dbInfo, binSize):\n",
    "    \n",
    "    (dbSingle, dbTriple, dbSize, dbThresh) = dbInfo\n",
    "    fps = extractFingerprints_midi(bscore)\n",
    "    \n",
    "    # construct histogram of offsets\n",
    "    h = {}\n",
    "    for i, fp in enumerate(fps):\n",
    "        if fp in dbSingle:\n",
    "            if len(dbSingle[fp]) < dbThresh: # rare fp value, use single fp matches\n",
    "                matches = dbSingle[fp] # list of (pieceNum, offset, fpNext, fpNextNext)\n",
    "            elif i < len(fps) - 2: # common fp value, use triple fp matches\n",
    "                fpTriple = fp + fps[i+1] // 2 + fps[i+2] // 4\n",
    "                if fpTriple in dbTriple:\n",
    "                    matches = dbTriple[fpTriple] # list of (pieceNum, offset)\n",
    "                else:\n",
    "                    continue\n",
    "            else: # common fp value but at end of sequence, ignore\n",
    "                continue\n",
    "            for m in matches:\n",
    "                pieceNum = m[0]\n",
    "                refOffset = m[1]\n",
    "                offsetDiff = (refOffset - i) // binSize\n",
    "                if pieceNum not in h:\n",
    "                    h[pieceNum] = {}\n",
    "                if offsetDiff not in h[pieceNum]:\n",
    "                    h[pieceNum][offsetDiff] = 0\n",
    "                h[pieceNum][offsetDiff] += 1\n",
    "    \n",
    "    # sort db items by match score\n",
    "    scores = [0] * (dbSize+1) # numbering starts from 1, ignore scores[0]\n",
    "    for pieceNum in range(1, dbSize + 1):\n",
    "        if pieceNum in h:\n",
    "            scores[pieceNum] = np.max([h[pieceNum][offset] for offset in h[pieceNum]])\n",
    "    \n",
    "    resultList = np.argsort(scores)[::-1] # sorted from highest score to lowest score\n",
    "    \n",
    "    return resultList, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFingerprints_midi(D):\n",
    "    '''\n",
    "    Extract list of fingerprint tuples given a MIDI bootleg score matrix.\n",
    "    '''\n",
    "    numBits = D.shape[0]\n",
    "    if D.shape[1] <= 1: # empty or contains single filler column\n",
    "        return []\n",
    "    assert numBits < 64, \"Number of bits must be less than 64.\"\n",
    "    mask = np.power(2, np.arange(numBits)).reshape((1,-1))\n",
    "    fps = np.squeeze(mask @ D)\n",
    "    \n",
    "    return fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_bscore_file = 'midi_feat/p1.pkl'\n",
    "sampleDur = 20\n",
    "nsamples = 2\n",
    "binSize = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_both, sorted_sharp, sorted_flat, scores_sharp, scores_flat = processMidiQuery(midi_bscore_file, dbInfo, sampleDur, nsamples, binSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process All Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processAllQueries(midi_bscore_dir, dbInfo, sampleDur, nsamples, binSize, outdir):\n",
    "    if not os.path.isdir(outdir):\n",
    "        os.makedirs(outdir)\n",
    "\n",
    "    numQueryFiles = len(glob.glob('{}/*.pkl'.format(midi_bscore_dir)))\n",
    "    for i in range(1, numQueryFiles+1):\n",
    "        midi_bscore_file = '{}/p{}.pkl'.format(midi_bscore_dir, i)\n",
    "        hyp_file = '{}/p{}.hyp'.format(outdir, i)\n",
    "        if os.path.exists(hyp_file):\n",
    "            print('Skipping {}'.format(midi_bscore_file))\n",
    "        else:\n",
    "            print('Processing {}'.format(midi_bscore_file))\n",
    "            processMidiQuery(midi_bscore_file, dbInfo, sampleDur, nsamples, binSize, hyp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_bscore_dir = 'midi_feat'\n",
    "sampleDur = 100\n",
    "nsamples = 10\n",
    "binSize = 10\n",
    "results_dir = 'hyps/sample{}'.format(sampleDur)\n",
    "#processAllQueries(midi_bscore_dir, dbInfo, sampleDur, nsamples, binSize, results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processAllQueries(midi_bscore_dir, dbInfo, 10, nsamples, binSize, 'hyps/sample10')\n",
    "processAllQueries(midi_bscore_dir, dbInfo, 20, nsamples, binSize, 'hyps/sample20')\n",
    "processAllQueries(midi_bscore_dir, dbInfo, 50, nsamples, binSize, 'hyps/sample50')\n",
    "processAllQueries(midi_bscore_dir, dbInfo, 100, nsamples, binSize, 'hyps/sample100')\n",
    "processAllQueries(midi_bscore_dir, dbInfo, 200, nsamples, binSize, 'hyps/sample200')\n",
    "processAllQueries(midi_bscore_dir, dbInfo, 500, nsamples, binSize, 'hyps/sample500')\n",
    "processAllQueries(midi_bscore_dir, dbInfo, 1000, nsamples, binSize, 'hyps/sample1000')\n",
    "processAllQueries(midi_bscore_dir, dbInfo, 100000, nsamples, binSize, 'hyps/sample100000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # process all queries\n",
    "# midi_bscore_dir = 'midi_feat'\n",
    "# #dbfile = 'dbAll.pkl'\n",
    "# sampleDur = 100\n",
    "# nsamples = 10\n",
    "# binSize = 10\n",
    "# results_dir = 'hyps/sample{}'.format(sampleDur)\n",
    "\n",
    "# # prep output directory\n",
    "# if not os.path.isdir(results_dir):\n",
    "#     os.makedirs(results_dir)\n",
    "\n",
    "# # number of cores to use\n",
    "# n_cores = 4 #multiprocessing.cpu_count()\n",
    "\n",
    "# # prep inputs for parallelization\n",
    "# inputs = []\n",
    "# numQueryFiles = len(glob.glob('{}/*.pkl'.format(midi_bscore_dir)))\n",
    "# for i in range(1, numQueryFiles+1):\n",
    "#     midi_bscore_file = '{}/p{}.pkl'.format(midi_bscore_dir, i)\n",
    "#     hyp_file = '{}/p{}.hyp'.format(results_dir, i)\n",
    "#     if os.path.exists(hyp_file):\n",
    "#         print('Skipping {}'.format(midi_bscore_file))\n",
    "#     else:\n",
    "#         inputs.append((midi_bscore_file, dbInfo, sampleDur, nsamples, binSize, hyp_file))\n",
    "\n",
    "# # process queries in parallel\n",
    "# pool = multiprocessing.Pool(processes=n_cores)\n",
    "# outputs = list(pool.starmap(processMidiQuery, inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SheetMidiRetrieval",
   "language": "python",
   "name": "sheetmidiretrieval"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
