{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.matlib import repmat\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageFilter, ImageChops\n",
    "import cv2\n",
    "from skimage import filters, measure\n",
    "from skimage.measure import label, regionprops\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.patches as mpatches\n",
    "from scipy.signal import convolve2d\n",
    "import pickle\n",
    "import librosa as lb\n",
    "import time\n",
    "import os\n",
    "import os.path\n",
    "import glob\n",
    "import subprocess\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert PDF to PNG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use ImageMagick to convert PDF files to PNG images.  It is important to have appropriate settings for ImageMagick to be able to process all files.  Here are the recommended settings\n",
    "- memory: 8GiB\n",
    "- map: 4GiB\n",
    "- disk: 8GiB\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "These settings can be change in e.g. /etc/ImageMagick-6/policy.xml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertPDF2PNG(pdffile, pngfile):\n",
    "    firstpage = pngfile[0:-4] + '-0.png'\n",
    "    if os.path.exists(pngfile) or os.path.exists(firstpage):\n",
    "        #print('Skipping {}'.format(os.path.basename(pdffile)))\n",
    "        pass\n",
    "    else:\n",
    "        outdir = os.path.dirname(pngfile)\n",
    "        if not os.path.isdir(outdir):\n",
    "            os.makedirs(outdir)\n",
    "        print('Converting {}'.format(os.path.basename(pdffile)))\n",
    "        subprocess.call(['convert', '-density', '300', '-alpha', 'remove', '-resize', '2550', pdffile, pngfile])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF to PNG conversion\n",
    "db_list = 'cfg_files/db.list' # list of database pdfs\n",
    "png_dir = 'data/png' # where to save converted png files\n",
    "n_cores = 28 #multiprocessing.cpu_count()\n",
    "\n",
    "# prep inputs for parallelization\n",
    "inputs = []\n",
    "with open(db_list, 'r') as f:\n",
    "    for line in f:\n",
    "        pdffile = line.strip() # data/pdf/p1.pdf\n",
    "        basename = os.path.splitext(os.path.basename(pdffile))[0]\n",
    "        outdir = '{}/{}'.format(png_dir, basename)\n",
    "        pngfile = '{}/{}.png'.format(outdir, basename)\n",
    "        inputs.append((pdffile, pngfile))\n",
    "\n",
    "# process queries in parallel\n",
    "pool = multiprocessing.Pool(processes=n_cores)\n",
    "outputs = list(pool.starmap(convertPDF2PNG, inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def renameSinglePageFiles(png_dir):\n",
    "    '''\n",
    "    If the pdf contains only 1 page, the name of the file will be p123.png, not p123-0.png.\n",
    "    to keep a consistent naming convention, we rename these to p123-0.png.\n",
    "    '''\n",
    "    for dirname in glob.glob('{}/*'.format(png_dir)):\n",
    "        pieceid = os.path.basename(dirname)\n",
    "        singlePageFilename = '{}/{}.png'.format(dirname, pieceid)\n",
    "        multiPageFilename = '{}/{}-0.png'.format(dirname, pieceid)\n",
    "        if os.path.exists(singlePageFilename):\n",
    "            os.rename(singlePageFilename, multiPageFilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renameSinglePageFiles(png_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagefile = 'data/png/p1/p1-0.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### system parameters ###\n",
    "\n",
    "# Pre-processing\n",
    "thumbnailW = 100  # bkgd lighting\n",
    "thumbnailH = 100\n",
    "thumbnailFilterSize = 5\n",
    "estLineSep_NumCols = 3\n",
    "estLineSep_LowerRange = 12 # adjusted from 25\n",
    "estLineSep_UpperRange = 30 # adjusted from 45\n",
    "estLineSep_Delta = 1\n",
    "targetLineSep = 10.0\n",
    "\n",
    "# Staff Line Features\n",
    "morphFilterHorizLineSize = 41\n",
    "notebarFiltLen = 3\n",
    "notebarRemoval = 0.9\n",
    "calcStaveFeatureMap_NumCols = 10\n",
    "calcStaveFeatureMap_LowerRange = 8.5\n",
    "calcStaveFeatureMap_UpperRange = 11.75\n",
    "calcStaveFeatureMap_Delta = 0.25\n",
    "\n",
    "# Notehead Detection\n",
    "morphFilterCircleSizeReduce = 5\n",
    "morphFilterCircleSizeExpand = 5\n",
    "#morphFilterCircleSize = 5\n",
    "notedetect_minarea = 50\n",
    "notedetect_maxarea = 200\n",
    "noteTemplateSize = 21\n",
    "notedetect_tol_ratio = .4\n",
    "chordBlock_minH = 1.25\n",
    "chordBlock_maxH = 4.25\n",
    "chordBlock_minW = .8\n",
    "chordBlock_maxW = 2.25\n",
    "chordBlock_minArea = 1.8\n",
    "chordBlock_maxArea = 4.5\n",
    "chordBlock_minNotes = 2\n",
    "chordBlock_maxNotes = 4\n",
    "\n",
    "# Staffline Detection\n",
    "maxDeltaRowInitial = 50\n",
    "minNumStaves = 6 # adjusted from 2\n",
    "maxNumStaves = 16 # adjusted from 12\n",
    "minStaveSeparation = 6 * targetLineSep\n",
    "maxDeltaRowRefined = 15\n",
    "\n",
    "# Group Staves\n",
    "morphFilterVertLineLength = 101\n",
    "morphFilterVertLineWidth = 7\n",
    "maxBarlineWidth = 15\n",
    "#maxBarlineLenFactor = .25\n",
    "\n",
    "# Generate Bootleg Score\n",
    "bootlegRepeatNotes = 1\n",
    "bootlegFiller = 0\n",
    "\n",
    "##########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing consists of two steps:\n",
    "- background subtraction to reduce effect of lighting conditions\n",
    "- interline normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pim1 = Image.open(imagefile).convert('L') # pim indicates PIL image object, im indicates raw pixel values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeBkgdLighting(pimg, filtsz=5, thumbnailW = 100, thumbnailH = 100):\n",
    "    tinyimg = pimg.copy()\n",
    "    tinyimg.thumbnail([thumbnailW, thumbnailH]) # resize to speed up\n",
    "    shadows = tinyimg.filter(ImageFilter.GaussianBlur(filtsz)).resize(pimg.size)\n",
    "    result = ImageChops.invert(ImageChops.subtract(shadows, pimg))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pim2 = removeBkgdLighting(pim1, thumbnailFilterSize, thumbnailW, thumbnailH)\n",
    "pim2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPenalizedCombFilter(linesep):\n",
    "    filt = np.zeros(int(np.round(linesep * 5)))\n",
    "    \n",
    "    # positive spikes\n",
    "    for i in range(5):\n",
    "        offset = int(np.round(.5*linesep + i*linesep))\n",
    "        filt[offset-1:offset+2] = 1.0\n",
    "    \n",
    "    # negative spikes\n",
    "    for i in range(6):\n",
    "        center = int(np.round(i*linesep))\n",
    "        startIdx = max(center - 1, 0)\n",
    "        endIdx = min(center + 2, len(filt))\n",
    "        filt[startIdx:endIdx] = -1.0\n",
    "        \n",
    "    return filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimateLineSep(pim, ncols, lrange, urange, delta):\n",
    "    \n",
    "    # break image into columns, calculate row medians for inner columns (exclude outermost columns)\n",
    "    img = 255 - np.array(pim)\n",
    "    imgHeight, imgWidth = img.shape\n",
    "    rowMedians = np.zeros((imgHeight, ncols))\n",
    "    colWidth = imgWidth // (ncols + 2)\n",
    "    for i in range(ncols):\n",
    "        rowMedians[:,i] = np.median(img[:,(i+1)*colWidth:(i+2)*colWidth], axis=1)\n",
    "    \n",
    "    # apply comb filters\n",
    "    lineseps = np.arange(lrange, urange, delta)\n",
    "    responses = np.zeros((len(lineseps), imgHeight, ncols))\n",
    "    for i, linesep in enumerate(lineseps):\n",
    "        filt = getPenalizedCombFilter(linesep).reshape((-1,1))\n",
    "        responses[i,:,:] = convolve2d(rowMedians, filt, mode = 'same')\n",
    "    \n",
    "    # find comb filter with strongest response\n",
    "    scores = np.sum(np.max(responses, axis=1), axis=1)\n",
    "    bestIdx = np.argmax(scores)\n",
    "    estLineSep = lineseps[bestIdx]\n",
    "    \n",
    "    return estLineSep, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linesep, scores = estimateLineSep(pim2, estLineSep_NumCols, estLineSep_LowerRange, estLineSep_UpperRange, estLineSep_Delta)\n",
    "linesep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcResizedDimensions(pim, estimatedLineSep, desiredLineSep):\n",
    "    curH, curW = pim.height, pim.width\n",
    "    scale_factor = 1.0 * desiredLineSep / estimatedLineSep\n",
    "    targetH = int(curH * scale_factor)\n",
    "    targetW = int(curW * scale_factor)    \n",
    "    return targetH, targetW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetH, targetW = calcResizedDimensions(pim2, linesep, targetLineSep)\n",
    "pim2 = pim2.resize((targetW, targetH))\n",
    "targetH, targetW, pim1.height, pim1.width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Staff Line Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNormImage(img):\n",
    "    X = 1 - np.array(img) / 255.0\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showGrayscaleImage(X, sz = (10,10), maxval = 1, inverted = True):\n",
    "    # by default assumes X is a normalized image between 0 (white) and 1 (black)\n",
    "    plt.figure(figsize = sz)\n",
    "    if inverted:\n",
    "        plt.imshow(maxval-X, cmap='gray')\n",
    "    else:\n",
    "        plt.imshow(X, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = getNormImage(pim2)\n",
    "showGrayscaleImage(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def morphFilterRectangle(arr, kernel_height, kernel_width):\n",
    "    kernel = np.ones((kernel_height, kernel_width),np.uint8)\n",
    "    result = cv2.erode(arr, kernel, iterations = 1)\n",
    "    result = cv2.dilate(result, kernel, iterations = 1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolateStaffLines(arr, kernel_len, notebarfilt_len, notebar_removal):\n",
    "    lines = morphFilterRectangle(arr, 1, kernel_len) # isolate horizontal lines\n",
    "    notebarsOnly = morphFilterRectangle(lines, notebarfilt_len, 1) # isolate thick notebars\n",
    "    result = np.clip(lines - notebar_removal*notebarsOnly, 0, None) # subtract out notebars\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hlines = isolateStaffLines(X2, morphFilterHorizLineSize, notebarFiltLen, notebarRemoval)\n",
    "showGrayscaleImage(hlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCombFilter(lineSep):\n",
    "    # generate comb filter of specified length\n",
    "    # e.g. if length is 44, then spikes at indices 0, 11, 22, 33, 44\n",
    "    # e.g. if length is 43, then spikes at 0 [1.0], 10 [.25], 11 [.75], 21 [.5], 22 [.5], 32 [.75], 33 [.25], 43 [1.0]\n",
    "    stavelen = int(np.ceil(4 * lineSep)) + 1\n",
    "    combfilt = np.zeros(stavelen)\n",
    "    for i in range(5):\n",
    "        idx = i * lineSep\n",
    "        idx_below = int(idx)\n",
    "        idx_above = idx_below + 1\n",
    "        remainder = idx - idx_below\n",
    "        combfilt[idx_below] = 1 - remainder\n",
    "        if idx_above < stavelen:\n",
    "            combfilt[idx_above] = remainder\n",
    "    return combfilt, stavelen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeStaveFeatureMap(img, ncols, lrange, urange, delta):\n",
    "    \n",
    "    # break image into columns, calculate row medians\n",
    "    imgHeight, imgWidth = img.shape\n",
    "    rowSums = np.zeros((imgHeight, ncols))\n",
    "    colWidth = int(np.ceil(imgWidth/ncols))\n",
    "    for i in range(ncols):\n",
    "        startCol = i * colWidth\n",
    "        endCol = min((i+1)*colWidth, imgWidth)\n",
    "        rowSums[:,i] = np.sum(img[:,startCol:endCol], axis=1)\n",
    "    \n",
    "    # apply comb filters\n",
    "    lineseps = np.arange(lrange, urange, delta)\n",
    "    maxFiltSize = int(np.ceil(4 * lineseps[-1])) + 1\n",
    "    featmap = np.zeros((len(lineseps), imgHeight - maxFiltSize + 1, ncols))\n",
    "    stavelens = np.zeros(len(lineseps), dtype=np.int)\n",
    "    for i, linesep in enumerate(lineseps):\n",
    "        filt, stavelen = getCombFilter(linesep)\n",
    "        padded = np.zeros((maxFiltSize, 1))\n",
    "        padded[0:len(filt),:] = filt.reshape((-1,1))\n",
    "        featmap[i,:,:] = convolve2d(rowSums, np.flipud(np.fliplr(padded)), mode = 'valid')\n",
    "        stavelens[i] = stavelen\n",
    "        \n",
    "    return featmap, stavelens, colWidth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featmap, stavelens, columnWidth = computeStaveFeatureMap(hlines, calcStaveFeatureMap_NumCols, calcStaveFeatureMap_LowerRange, calcStaveFeatureMap_UpperRange, calcStaveFeatureMap_Delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notehead Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def morphFilterCircle(pimg, sz_reduce = 5, sz_expand = 0):\n",
    "    kernel_reduce = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (sz_reduce, sz_reduce))\n",
    "    result = cv2.dilate(np.array(pimg), kernel_reduce, iterations = 1)\n",
    "    if sz_expand > 0:\n",
    "        kernel_expand = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (sz_expand, sz_expand))\n",
    "        result = cv2.erode(result, kernel_expand, iterations = 1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im3 = morphFilterCircle(pim2, morphFilterCircleSizeReduce, morphFilterCircleSizeExpand) # from here on use raw pixel values, not PIL image object\n",
    "showGrayscaleImage(im3, maxval = 255, inverted = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectNoteheadBlobs(img, minarea, maxarea):\n",
    "    \n",
    "    # define blob detector\n",
    "    params = cv2.SimpleBlobDetector_Params()\n",
    "\n",
    "    # Change thresholds\n",
    "    # params.minThreshold = 100;\n",
    "    # params.maxThreshold = 200;\n",
    "\n",
    "    # Filter by Area\n",
    "    # params.filterByArea = True\n",
    "    params.minArea = minarea\n",
    "    params.maxArea = maxarea\n",
    "\n",
    "    # Filter by Circularity\n",
    "    # params.filterByCircularity = True\n",
    "    # params.minCircularity = 0.1\n",
    "\n",
    "    # Filter by Convexity\n",
    "    # params.filterByConvexity = True\n",
    "    # params.minConvexity = 0.87\n",
    "\n",
    "    # Filter by Inertia\n",
    "    # params.filterByInertia = True\n",
    "    # params.minInertiaRatio = 0.01\n",
    "\n",
    "    # Create a detector with the parameters\n",
    "    detector = cv2.SimpleBlobDetector_create(params)\n",
    "    \n",
    "    keypoints = detector.detect(img)\n",
    "    im_with_keypoints = cv2.drawKeypoints(np.array(img), keypoints, np.array([]), (0,0,255), cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    \n",
    "    return keypoints, im_with_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showColorImage(X, sz = (10,10)):\n",
    "    plt.figure(figsize = sz)\n",
    "    plt.imshow(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints, im_with_keypoints = detectNoteheadBlobs(im3, notedetect_minarea, notedetect_maxarea)\n",
    "showColorImage(im_with_keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNoteTemplate(arr, keypoints, sz = 21):\n",
    "    template = np.zeros((sz,sz))\n",
    "    L = (sz - 1)//2\n",
    "    #crops = []\n",
    "    numCrops = 0\n",
    "    for k in keypoints:\n",
    "        xloc = int(np.round(k.pt[0])) # col\n",
    "        yloc = int(np.round(k.pt[1])) # row\n",
    "        if xloc - L >= 0 and xloc + L + 1 <= arr.shape[1] and yloc - L >= 0 and yloc + L + 1 <= arr.shape[0]:\n",
    "            crop = arr[yloc-L:yloc+L+1,xloc-L:xloc+L+1]\n",
    "            #crops.append(crop)\n",
    "            template += crop\n",
    "            numCrops += 1\n",
    "    if numCrops > 0:\n",
    "        template = template / numCrops\n",
    "    #template = template - np.mean(template.ravel()) # will be used as a filter, so make zero mean\n",
    "    return template, numCrops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3 = getNormImage(im3) # im indicates grayscale [0, 255], X indicates [0, 1] inverted grayscale\n",
    "ntemplate, numCrops = getNoteTemplate(X3, keypoints, noteTemplateSize)\n",
    "showGrayscaleImage(ntemplate, (3,3), maxval = 1, inverted = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptiveNoteheadDetect(arr, template, noteTolRatio, chordBlockSpecs):\n",
    "    #filtered = convolve2d(arr, np.flipud(np.fliplr(template)), mode='same', boundary='symm')\n",
    "    binarized, _ = binarize_otsu(arr)\n",
    "    labels = measure.label(binarized)\n",
    "    notes = []\n",
    "    if template.max() == 0: # no noteheads detected\n",
    "        return notes, binarized\n",
    "    templateSpecs = getNoteTemplateSpecs(template)\n",
    "    for region in regionprops(labels):\n",
    "        if isValidNotehead(region, noteTolRatio, templateSpecs):\n",
    "            notes.append(region.bbox)\n",
    "        elif isValidChordBlock(region, chordBlockSpecs, templateSpecs):\n",
    "            chordNotes = extractNotesFromChordBlock(region, templateSpecs)\n",
    "            notes.extend(chordNotes)\n",
    "    return notes, binarized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize_otsu(img):\n",
    "    arr = np.array(img)\n",
    "    thresh = filters.threshold_otsu(arr)\n",
    "    binarized = arr > thresh\n",
    "    return binarized, thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNoteTemplateSpecs(template):\n",
    "    _, thresh = binarize_otsu(template)\n",
    "    binarized = template > thresh\n",
    "    labels = measure.label(binarized)\n",
    "    maxH, maxW, maxArea = (0, 0, 0)\n",
    "    for region in regionprops(labels):\n",
    "        curH = region.bbox[2] - region.bbox[0]\n",
    "        curW = region.bbox[3] - region.bbox[1]\n",
    "        curArea = region.area\n",
    "        if curArea > maxArea:\n",
    "            maxArea = curArea\n",
    "            maxH = curH\n",
    "            maxW = curW\n",
    "    return (maxH, maxW, maxArea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isValidNotehead(region, tol_ratio, templateSpecs):\n",
    "    templateH, templateW, templateArea = templateSpecs\n",
    "    max_ratio = 1 + tol_ratio\n",
    "    min_ratio = 1 / (1 + tol_ratio)\n",
    "    curH = region.bbox[2] - region.bbox[0]\n",
    "    curW = region.bbox[3] - region.bbox[1]\n",
    "    curArea = region.area\n",
    "    curRatio = 1.0 * curH / curW\n",
    "    templateRatio = 1.0 * templateH / templateW\n",
    "    validH = curH < templateH * max_ratio and curH > templateH * min_ratio\n",
    "    validW = curW < templateW * max_ratio and curW > templateW * min_ratio\n",
    "    validArea = curArea < templateArea * max_ratio * max_ratio and curArea > templateArea * min_ratio * min_ratio\n",
    "    validRatio = curRatio < templateRatio * max_ratio and curRatio > templateRatio * min_ratio\n",
    "    result = validH and validW and validRatio and validArea\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isValidChordBlock(region, params, templateSpecs):\n",
    "    templateH, templateW, templateArea = templateSpecs\n",
    "    minH, maxH, minW, maxW, minArea, maxArea, minNotes, maxNotes = params\n",
    "    curH = region.bbox[2] - region.bbox[0]\n",
    "    curW = region.bbox[3] - region.bbox[1]\n",
    "    curArea = region.area\n",
    "    curNotes = int(np.round(curArea / templateArea))\n",
    "    validH = curH >= minH * templateH and curH <= maxH * templateH\n",
    "    validW = curW >= minW * templateW and curW <= maxW * templateW\n",
    "    validArea = curArea >= minArea * templateArea and curArea <= maxArea * templateArea\n",
    "    validNotes = curNotes >= minNotes and curNotes <= maxNotes\n",
    "    result = validH and validW and validArea and validNotes\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractNotesFromChordBlock(region, templateSpecs):\n",
    "    # use kmeans to estimate note centers\n",
    "    templateH, templateW, templateArea = templateSpecs\n",
    "    numNotes = int(np.round(region.area / templateArea))\n",
    "    regionCoords = np.array(region.coords)\n",
    "    kmeans = KMeans(n_clusters=numNotes, n_init = 1, random_state = 0).fit(regionCoords)\n",
    "    bboxes = []\n",
    "    for (r,c) in kmeans.cluster_centers_:\n",
    "        rmin = int(np.round(r - templateH/2))\n",
    "        rmax = int(np.round(r + templateH/2))\n",
    "        cmin = int(np.round(c - templateW/2))\n",
    "        cmax = int(np.round(c + templateW/2))\n",
    "        bboxes.append((rmin, cmin, rmax, cmax))\n",
    "    return bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeLabels(img, bboxes):\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.imshow(img)\n",
    "    \n",
    "    for (minr, minc, maxr, maxc) in bboxes:\n",
    "        rect = mpatches.Rectangle((minc, minr), maxc - minc, maxr - minr, fill=False, edgecolor='red', linewidth=2)\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    ax.set_axis_off()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chordBlockSpecs = (chordBlock_minH, chordBlock_maxH, chordBlock_minW, chordBlock_maxW, chordBlock_minArea, chordBlock_maxArea, chordBlock_minNotes, chordBlock_maxNotes)\n",
    "notes, img_binarized_notes = adaptiveNoteheadDetect(X3, ntemplate, notedetect_tol_ratio, chordBlockSpecs)\n",
    "#showGrayscaleImage(img_binarized_notes)\n",
    "visualizeLabels(img_binarized_notes, notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNoteheadInfo(bboxes):\n",
    "    nhlocs = [(.5*(bbox[0] + bbox[2]), .5*(bbox[1] + bbox[3])) for bbox in bboxes]\n",
    "    nhlens = [(bbox[2] - bbox[0]) for bbox in bboxes]\n",
    "    nhwidths = [(bbox[3] - bbox[1]) for bbox in bboxes]\n",
    "    nhlen_est = int(np.ceil(np.mean(nhlens)))\n",
    "    nhwidth_est = int(np.ceil(np.mean(nhwidths)))\n",
    "    return nhlocs, nhlen_est, nhwidth_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nhlocs, nhlen_est, nhwidth_est = getNoteheadInfo(notes)\n",
    "nhlen_est, nhwidth_est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infer Note Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEstStaffLineLocs(featmap, nhlocs, stavelens, colWidth, deltaRowMax, globalOffset = 0):\n",
    "    preds = []\n",
    "    if np.isscalar(globalOffset):\n",
    "        globalOffset = [globalOffset] * len(nhlocs)\n",
    "    for i, nhloc in enumerate(nhlocs):\n",
    "        r = int(np.round(nhloc[0]))\n",
    "        c = int(np.round(nhloc[1]))\n",
    "        rupper = min(r + deltaRowMax + 1 + globalOffset[i], featmap.shape[1])\n",
    "        rlower = max(r - deltaRowMax + globalOffset[i], 0)\n",
    "        featmapIdx = c // colWidth\n",
    "        regCurrent = np.squeeze(featmap[:, rlower:rupper, featmapIdx])\n",
    "        mapidx, roffset = np.unravel_index(regCurrent.argmax(), regCurrent.shape)    \n",
    "        rstart = rlower + roffset\n",
    "        rend = rstart + stavelens[mapidx] - 1\n",
    "        preds.append((rstart, rend, c, r, mapidx))\n",
    "        \n",
    "    sfiltlen = int(np.round(np.median([stavelens[tup[4]] for tup in preds])))\n",
    "    return preds, sfiltlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeEstStaffLines(preds, arr):\n",
    "    showGrayscaleImage(arr, (15,15))\n",
    "    rows1 = np.array([pred[0] for pred in preds]) # top staff line\n",
    "    rows2 = np.array([pred[1] for pred in preds]) # bottom staff line\n",
    "    cols = np.array([pred[2] for pred in preds]) # nh col\n",
    "    rows3 = np.array([pred[3] for pred in preds]) # nh row\n",
    "    plt.scatter(cols, rows1, c = 'r', s = 3)\n",
    "    plt.scatter(cols, rows2, c = 'b', s = 3)\n",
    "    plt.scatter(cols, rows3, c = 'y', s = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estStaffLineLocs, sfiltlen = getEstStaffLineLocs(featmap, nhlocs, stavelens, columnWidth, maxDeltaRowInitial, int(-2*targetLineSep))\n",
    "visualizeEstStaffLines(estStaffLineLocs, hlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimateStaffMidpoints(preds, clustersMin, clustersMax, threshold):\n",
    "    r = np.array([.5*(tup[0] + tup[1]) for tup in preds]) # midpts of estimated stave locations\n",
    "    models = []\n",
    "    for numClusters in range(clustersMin, clustersMax + 1):\n",
    "        kmeans = KMeans(n_clusters=numClusters, n_init=1, random_state = 0).fit(r.reshape(-1,1))\n",
    "        sorted_list = np.array(sorted(np.squeeze(kmeans.cluster_centers_)))\n",
    "        mindiff = np.min(sorted_list[1:] - sorted_list[0:-1])\n",
    "        if numClusters > clustersMin and mindiff < threshold:\n",
    "            break\n",
    "        models.append(kmeans)\n",
    "    staffMidpts = np.sort(np.squeeze(models[-1].cluster_centers_))\n",
    "    return staffMidpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debugStaffMidpointClustering(preds):\n",
    "    r = np.array([.5*(tup[0] + tup[1]) for tup in preds]) # midpts of estimated stave locations\n",
    "    inertias = []\n",
    "    mindiffs = []\n",
    "    clusterRange = np.arange(2,12)\n",
    "    for numClusters in clusterRange:\n",
    "        kmeans = KMeans(n_clusters=numClusters, n_init=1, random_state = 0).fit(r.reshape(-1,1))\n",
    "        inertias.append(kmeans.inertia_)\n",
    "        sorted_list = np.array(sorted(np.squeeze(kmeans.cluster_centers_)))\n",
    "        diffs = sorted_list[1:] - sorted_list[0:-1]\n",
    "        mindiffs.append(np.min(diffs))\n",
    "    plt.subplot(211)\n",
    "    plt.plot(clusterRange, np.log(inertias))\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.ylabel('Inertia')\n",
    "    plt.subplot(212)\n",
    "    plt.plot(clusterRange, mindiffs)\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.ylabel('Min Centroid Separation')\n",
    "    plt.axhline(60, color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeStaffMidpointClustering(preds, centers):\n",
    "    r = np.array([.5*(tup[0] + tup[1]) for tup in preds]) # midpts of estimated stave locations\n",
    "    plt.plot(r, np.random.uniform(size = len(r)), '.')\n",
    "    for center in centers:\n",
    "        plt.axvline(x=center, color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staveMidpts = estimateStaffMidpoints(estStaffLineLocs, minNumStaves, maxNumStaves, minStaveSeparation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugStaffMidpointClustering(estStaffLineLocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizeStaffMidpointClustering(estStaffLineLocs, staveMidpts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assignNoteheadsToStaves(nhlocs, staveCenters):\n",
    "    nhrows = np.matlib.repmat([tup[0] for tup in nhlocs], len(staveCenters), 1)\n",
    "    centers = np.matlib.repmat(staveCenters.reshape((-1,1)), 1, len(nhlocs))\n",
    "    staveIdxs = np.argmin(np.abs(nhrows - centers), axis=0)\n",
    "    offsets = staveCenters[staveIdxs] - nhrows[0,:] # row offset between note and staff midpoint\n",
    "    return staveIdxs, offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeClusters(arr, nhlocs, clusters):\n",
    "    showGrayscaleImage(arr)\n",
    "    rows = np.array([tup[0] for tup in nhlocs])\n",
    "    cols = np.array([tup[1] for tup in nhlocs])\n",
    "    plt.scatter(cols, rows, c=clusters)\n",
    "    for i in range(len(clusters)):\n",
    "        plt.text(cols[i], rows[i] - 15, str(clusters[i]), fontsize = 12, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staveIdxs, nhRowOffsets = assignNoteheadsToStaves(nhlocs, staveMidpts)\n",
    "visualizeClusters(X2, nhlocs, staveIdxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estStaffLineLocs, sfiltlen = getEstStaffLineLocs(featmap, nhlocs, stavelens, columnWidth, maxDeltaRowRefined, (nhRowOffsets - 2*targetLineSep).astype(np.int))\n",
    "visualizeEstStaffLines(estStaffLineLocs, hlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimateNoteLabels(preds):\n",
    "    nhvals = [] # estimated note labels\n",
    "    for i, (rstart, rend, c, r, filtidx) in enumerate(preds):       \n",
    "        # if a stave has height L, there are 8 stave locations in (L-1) pixel rows\n",
    "        staveMidpt = .5 * (rstart + rend)\n",
    "        noteStaveLoc = -1.0 * (r - staveMidpt) * 8 / (rend - rstart)\n",
    "        nhval = int(np.round(noteStaveLoc))\n",
    "        nhvals.append(nhval)\n",
    "    return nhvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeNoteLabels(arr, vals, locs):\n",
    "    showGrayscaleImage(arr)\n",
    "    rows = np.array([loc[0] for loc in locs])\n",
    "    cols = np.array([loc[1] for loc in locs])\n",
    "    plt.scatter(cols, rows, color='blue')\n",
    "    for i in range(len(rows)):\n",
    "        plt.text(cols[i], rows[i] - 15, str(vals[i]), fontsize = 12, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nhvals = estimateNoteLabels(estStaffLineLocs)\n",
    "visualizeNoteLabels(X2, nhvals, nhlocs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster staves & noteheads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolateBarlines(im, morphFilterVertLineLength, morphFilterVertLineWidth, maxBarlineWidth):\n",
    "    hkernel = np.ones((1, morphFilterVertLineWidth), np.uint8) # dilate first to catch warped barlines\n",
    "    vlines = cv2.dilate(im, hkernel, iterations = 1)\n",
    "    vlines = morphFilterRectangle(vlines, morphFilterVertLineLength, 1) # then filter for tall vertical lines\n",
    "    nonbarlines = morphFilterRectangle(vlines, 1, maxBarlineWidth)\n",
    "    vlines = np.clip(vlines - nonbarlines, 0, 1)\n",
    "    return vlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vlines = isolateBarlines(X2, morphFilterVertLineLength, morphFilterVertLineWidth, maxBarlineWidth)\n",
    "showGrayscaleImage(vlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determineStaveGrouping(staveMidpts, vlines):\n",
    "    \n",
    "    N = len(staveMidpts)\n",
    "    rowSums = np.sum(vlines, axis=1)\n",
    "    \n",
    "    # grouping A: 0-1, 2-3, 4-5, ...\n",
    "    elems_A = []\n",
    "    map_A = {}\n",
    "    for i, staveIdx in enumerate(np.arange(0, N, 2)):\n",
    "        if staveIdx+1 < N:\n",
    "            startRow = int(staveMidpts[staveIdx])\n",
    "            endRow = int(staveMidpts[staveIdx+1]) + 1\n",
    "            elems_A.extend(rowSums[startRow:endRow])\n",
    "            map_A[staveIdx] = staveIdx\n",
    "            map_A[staveIdx+1] = staveIdx + 1\n",
    "        else:\n",
    "            map_A[staveIdx] = -1 # unpaired stave\n",
    "    \n",
    "    # grouping B: 1-2, 3-4, 5-6, ...\n",
    "    elems_B = []\n",
    "    map_B = {}\n",
    "    map_B[0] = -1 \n",
    "    for i, staveIdx in enumerate(np.arange(1, N, 2)):\n",
    "        if staveIdx+1 < N:\n",
    "            startRow = int(staveMidpts[staveIdx])\n",
    "            endRow = int(staveMidpts[staveIdx+1]) + 1\n",
    "            elems_B.extend(rowSums[startRow:endRow])\n",
    "            map_B[staveIdx] = staveIdx - 1\n",
    "            map_B[staveIdx + 1] = staveIdx\n",
    "        else:\n",
    "            map_B[staveIdx] = -1\n",
    "    \n",
    "    if N > 2:\n",
    "        evidence_A = np.median(elems_A)\n",
    "        evidence_B = np.median(elems_B)\n",
    "        if evidence_A > evidence_B:\n",
    "            mapping = map_A\n",
    "        else:\n",
    "            mapping = map_B\n",
    "    else:\n",
    "        evidence_A = np.median(elems_A)\n",
    "        evidence_B = 0\n",
    "        mapping = map_A\n",
    "    \n",
    "    return mapping, (evidence_A, evidence_B, elems_A, elems_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staveMapping, evidence = determineStaveGrouping(staveMidpts, vlines)\n",
    "np.median(evidence[2]), np.median(evidence[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debugStaveGrouping(vlines, staveCenters):\n",
    "    plt.plot(np.sum(vlines, axis=1))\n",
    "    for m in staveCenters:\n",
    "        plt.axvline(m, color = 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugStaveGrouping(vlines, staveMidpts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusterNoteheads(staveIdxs, mapping):\n",
    "    clusterIdxs = [mapping[staveIdx] for staveIdx in staveIdxs]\n",
    "    maxClusterIdx = np.max(np.array(clusterIdxs))\n",
    "    clusterPairs = []\n",
    "    for i in range(0, maxClusterIdx, 2):\n",
    "        clusterPairs.append((i,i+1))\n",
    "    return clusterIdxs, clusterPairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nhclusters, clusterPairs = clusterNoteheads(staveIdxs, staveMapping)\n",
    "visualizeClusters(X2, nhlocs, nhclusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Bootleg Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateSingleBootlegLine(nhdata, clusterR, clusterL, minColDiff, repeatNotes = 1, filler = 1):\n",
    "    notes = [tup for tup in nhdata if tup[3] == clusterR or tup[3] == clusterL]\n",
    "    notes = sorted(notes, key = lambda tup: (tup[1], tup[0])) # sort by column, then row\n",
    "    collapsed = collapseSimultaneousEvents(notes, minColDiff) # list of (rows, cols, vals, clusters)\n",
    "    bscore, eventIndices, staffLinesBoth, _, _ = constructBootlegScore(collapsed, clusterR, clusterL, repeatNotes, filler)\n",
    "    return bscore, collapsed, eventIndices, staffLinesBoth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapseSimultaneousEvents(notes, minColDiff):\n",
    "    assigned = np.zeros(len(notes), dtype=bool)\n",
    "    events = [] # list of simultaneous note events\n",
    "    for i, (row, col, val, cluster) in enumerate(notes):\n",
    "        if assigned[i]: # has already been assigned\n",
    "            continue\n",
    "        rows = [row] # new event\n",
    "        cols = [col]\n",
    "        vals = [val]\n",
    "        clusters = [cluster]\n",
    "        assigned[i] = True\n",
    "        for j in range(i+1, len(notes)):\n",
    "            nrow, ncol, nval, ncluster = notes[j]\n",
    "            if ncol - col < minColDiff: # assign to same event if close\n",
    "                rows.append(nrow)\n",
    "                cols.append(ncol)\n",
    "                vals.append(nval)\n",
    "                clusters.append(ncluster)\n",
    "                assigned[j] = True\n",
    "            else:\n",
    "                break\n",
    "        events.append((rows, cols, vals, clusters))\n",
    "    \n",
    "    assert(np.all(assigned))\n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constructBootlegScore(noteEvents, clusterIndexRH, clusterIndexLH, repeatNotes = 1, filler = 1):\n",
    "    # note that this has to match generateBootlegScore() in the previous notebook!\n",
    "    rh_dim = 34 # E3 to C8 (inclusive)\n",
    "    lh_dim = 28 # A1 to G4 (inclusive)\n",
    "    rh = [] # list of arrays of size rh_dim\n",
    "    lh = [] # list of arrays of size lh_dim\n",
    "    eventIndices = [] # index of corresponding simultaneous note event\n",
    "    for i, (rows, cols, vals, clusters) in enumerate(noteEvents):\n",
    "        \n",
    "        # insert empty filler columns between note events\n",
    "        if i > 0:\n",
    "            for j in range(filler):\n",
    "                rh.append(np.zeros((rh_dim,1)))\n",
    "                lh.append(np.zeros((lh_dim,1)))\n",
    "                eventIndices.append(i-1) # assign filler to previous event\n",
    "\n",
    "        # insert note events columns\n",
    "        rhvec, lhvec = getNoteheadPlacement(vals, clusters, rh_dim, lh_dim, clusterIndexRH, clusterIndexLH)\n",
    "        for j in range(repeatNotes):\n",
    "            rh.append(rhvec)\n",
    "            lh.append(lhvec)\n",
    "            eventIndices.append(i)\n",
    "    rh = np.squeeze(np.array(rh)).reshape((-1, rh_dim)).T # reshape handles case when len(rh) == 1\n",
    "    lh = np.squeeze(np.array(lh)).reshape((-1, lh_dim)).T\n",
    "    both = np.vstack((lh, rh))\n",
    "    staffLinesRH = [7,9,11,13,15]\n",
    "    staffLinesLH = [13,15,17,19,21]\n",
    "    staffLinesBoth = [13,15,17,19,21,35,37,39,41,43]\n",
    "    return both, eventIndices, staffLinesBoth, (rh, staffLinesRH), (lh, staffLinesLH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNoteheadPlacement(vals, clusters, rdim, ldim, clusterRH, clusterLH):\n",
    "    rhvec = np.zeros((rdim, 1))\n",
    "    lhvec = np.zeros((ldim, 1))\n",
    "    assert(clusterLH == clusterRH + 1)\n",
    "    for (val, cluster) in zip(vals, clusters):\n",
    "        if cluster == clusterRH:\n",
    "            idx = val + 11\n",
    "            if idx >= 0 and idx < rdim:\n",
    "                rhvec[idx, 0] = 1\n",
    "        elif cluster == clusterLH:\n",
    "            idx = val + 17\n",
    "            if idx >= 0 and idx < ldim:\n",
    "                lhvec[idx, 0] = 1\n",
    "        else:\n",
    "            print(\"Invalid cluster: {} (LH {}, RH {})\".format(cluster, clusterLH, clusterRH))\n",
    "            sys.exit(1)\n",
    "    return rhvec, lhvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeBootlegScore(bs, lines):\n",
    "    plt.figure(figsize = (10,10))\n",
    "    plt.imshow(1 - bs, cmap = 'gray', origin = 'lower')\n",
    "    for l in range(1, bs.shape[0], 2):\n",
    "        plt.axhline(l, c = 'grey')\n",
    "    for l in lines:\n",
    "        plt.axhline(l, c = 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of (row, col, value, cluster) tuples\n",
    "nhdata = [(int(np.round(nhlocs[i][0])), int(np.round(nhlocs[i][1])), nhvals[i], nhclusters[i]) for i in range(len(nhlocs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bscore, events, eventIndices, staffLinesBoth = generateSingleBootlegLine(nhdata, clusterR = 0, clusterL = 1, minColDiff = nhwidth_est, repeatNotes = 1, filler = 1)\n",
    "#visualizeBootlegScore(bscore, staffLinesBoth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateImageBootlegScore(nhdata, pairings, repeatNotes = 1, filler = 1, minColDiff = 10):\n",
    "    allScores = []\n",
    "    allEvents = []\n",
    "    globIndices = []\n",
    "    eventCount = 0\n",
    "    if len(pairings) == 0:\n",
    "        return None, None, None, None\n",
    "    for i, (clusterR, clusterL) in enumerate(pairings):\n",
    "        score, events, eventIndices, staffLinesBoth = generateSingleBootlegLine(nhdata, clusterR, clusterL, minColDiff, repeatNotes, filler)\n",
    "        allScores.append(score)\n",
    "        allEvents.extend(events)\n",
    "        globIndices.extend([idx + eventCount for idx in eventIndices])\n",
    "        if filler > 0 and i < len(pairings) - 1:\n",
    "            allScores.append(np.zeros((score.shape[0], filler))) # append filler columns between bootleg scores\n",
    "            globIndices.extend([globIndices[-1]] * filler) # map filler columns to last event index\n",
    "        eventCount += len(events)\n",
    "    panorama = np.hstack(allScores)\n",
    "    return panorama, allEvents, globIndices, staffLinesBoth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeLongBootlegScore(bs, lines, chunksz = 150):\n",
    "    chunks = bs.shape[1] // chunksz + 1\n",
    "    for i in range(chunks):\n",
    "        startcol = i * chunksz\n",
    "        endcol = min((i + 1)*chunksz, bs.shape[1])\n",
    "        visualizeBootlegScore(bs[:,startcol:endcol], lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bscore_query, events, eventIndices, staffLinesBoth = generateImageBootlegScore(nhdata, clusterPairs, bootlegRepeatNotes, bootlegFiller, minColDiff = nhwidth_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizeLongBootlegScore(bscore_query, staffLinesBoth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process All Sheet Music Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processImageFile(imagefile, outfile):\n",
    "\n",
    "    ### system parameters ###\n",
    "    \n",
    "    # Pre-processing\n",
    "    thumbnailW = 100  # bkgd lighting\n",
    "    thumbnailH = 100\n",
    "    thumbnailFilterSize = 5\n",
    "    estLineSep_NumCols = 3\n",
    "    estLineSep_LowerRange = 12 # adjusted from 25\n",
    "    estLineSep_UpperRange = 30 # adjusted from 45\n",
    "    estLineSep_Delta = 1\n",
    "    targetLineSep = 10.0\n",
    "\n",
    "    # Staff Line Features\n",
    "    morphFilterHorizLineSize = 41\n",
    "    notebarFiltLen = 3\n",
    "    notebarRemoval = 0.9\n",
    "    calcStaveFeatureMap_NumCols = 10\n",
    "    calcStaveFeatureMap_LowerRange = 8.5\n",
    "    calcStaveFeatureMap_UpperRange = 11.75\n",
    "    calcStaveFeatureMap_Delta = 0.25\n",
    "\n",
    "    # Notehead Detection\n",
    "    morphFilterCircleSizeReduce = 5\n",
    "    morphFilterCircleSizeExpand = 5\n",
    "    #morphFilterCircleSize = 5\n",
    "    notedetect_minarea = 50\n",
    "    notedetect_maxarea = 200\n",
    "    noteTemplateSize = 21\n",
    "    notedetect_tol_ratio = .4\n",
    "    chordBlock_minH = 1.25\n",
    "    chordBlock_maxH = 4.25\n",
    "    chordBlock_minW = .8\n",
    "    chordBlock_maxW = 2.25\n",
    "    chordBlock_minArea = 1.8\n",
    "    chordBlock_maxArea = 4.5\n",
    "    chordBlock_minNotes = 2\n",
    "    chordBlock_maxNotes = 4\n",
    "\n",
    "    # Staffline Detection\n",
    "    maxDeltaRowInitial = 50\n",
    "    minNumStaves = 6 # adjusted from 2\n",
    "    maxNumStaves = 16 # adjusted from 12\n",
    "    minStaveSeparation = 6 * targetLineSep\n",
    "    maxDeltaRowRefined = 15\n",
    "\n",
    "    # Group Staves\n",
    "    morphFilterVertLineLength = 101\n",
    "    morphFilterVertLineWidth = 7\n",
    "    maxBarlineWidth = 15\n",
    "    #maxBarlineLenFactor = .25\n",
    "\n",
    "    # Generate Bootleg Score\n",
    "    bootlegRepeatNotes = 1\n",
    "    bootlegFiller = 0\n",
    "\n",
    "    ##########################\n",
    "    \n",
    "    print(\"Processing {}\".format(imagefile))\n",
    "    profileStart = time.time()\n",
    "\n",
    "    # pre-processing\n",
    "    try:\n",
    "        pim1 = Image.open(imagefile).convert('L') # pim indicates PIL image object, im indicates raw pixel values\n",
    "    except:\n",
    "        if os.path.exists(imagefile):\n",
    "            saveEmptyResult(outfile, 'cannot open file')\n",
    "        else:\n",
    "            saveEmptyResult(outfile, 'imagefile not found')\n",
    "        return\n",
    "    pim2 = removeBkgdLighting(pim1, thumbnailFilterSize, thumbnailW, thumbnailH)\n",
    "    linesep, scores = estimateLineSep(pim2, estLineSep_NumCols, estLineSep_LowerRange, estLineSep_UpperRange, estLineSep_Delta)\n",
    "    targetH, targetW = calcResizedDimensions(pim2, linesep, targetLineSep)\n",
    "    pim2 = pim2.resize((targetW, targetH))\n",
    "    scale_factor = pim1.height / targetH\n",
    "\n",
    "    # staff line features\n",
    "    X2 = getNormImage(pim2)\n",
    "    hlines = isolateStaffLines(X2, morphFilterHorizLineSize, notebarFiltLen, notebarRemoval)\n",
    "    featmap, stavelens, columnWidth = computeStaveFeatureMap(hlines, calcStaveFeatureMap_NumCols, calcStaveFeatureMap_LowerRange, calcStaveFeatureMap_UpperRange, calcStaveFeatureMap_Delta)\n",
    "    \n",
    "    # notehead detection\n",
    "    im3 = morphFilterCircle(pim2, morphFilterCircleSizeReduce, morphFilterCircleSizeExpand)\n",
    "    keypoints, im_with_keypoints = detectNoteheadBlobs(im3, notedetect_minarea, notedetect_maxarea)\n",
    "    if len(keypoints) == 0:\n",
    "        saveEmptyResult(outfile, 'no keypoints detected')\n",
    "        return\n",
    "    X3 = getNormImage(im3) # im indicates grayscale [0, 255], X indicates [0, 1] inverted grayscale\n",
    "    ntemplate, numCrops = getNoteTemplate(X3, keypoints, noteTemplateSize)\n",
    "    chordBlockSpecs = (chordBlock_minH, chordBlock_maxH, chordBlock_minW, chordBlock_maxW, chordBlock_minArea, chordBlock_maxArea, chordBlock_minNotes, chordBlock_maxNotes)\n",
    "    notes, img_binarized_notes = adaptiveNoteheadDetect(X3, ntemplate, notedetect_tol_ratio, chordBlockSpecs)\n",
    "    if len(notes) < maxNumStaves: # if few or no notes detected, stop early (avoids later errors during kmeans clustering)\n",
    "        saveEmptyResult(outfile, 'too few noteheads')\n",
    "        return\n",
    "    nhlocs, nhlen_est, nhwidth_est = getNoteheadInfo(notes)\n",
    "    \n",
    "    # infer note values\n",
    "    estStaffLineLocs, sfiltlen = getEstStaffLineLocs(featmap, nhlocs, stavelens, columnWidth, maxDeltaRowInitial, int(-2*targetLineSep))\n",
    "    staveMidpts = estimateStaffMidpoints(estStaffLineLocs, minNumStaves, maxNumStaves, minStaveSeparation)\n",
    "    staveIdxs, nhRowOffsets = assignNoteheadsToStaves(nhlocs, staveMidpts)\n",
    "    estStaffLineLocs, sfiltlen = getEstStaffLineLocs(featmap, nhlocs, stavelens, columnWidth, maxDeltaRowRefined, (nhRowOffsets - 2*targetLineSep).astype(np.int))    \n",
    "    nhvals = estimateNoteLabels(estStaffLineLocs)\n",
    "    \n",
    "    # cluster noteheads & staves\n",
    "    vlines = isolateBarlines(X2, morphFilterVertLineLength, morphFilterVertLineWidth, maxBarlineWidth)\n",
    "    staveMapping, evidence = determineStaveGrouping(staveMidpts, vlines)\n",
    "    nhclusters, clusterPairs = clusterNoteheads(staveIdxs, staveMapping)\n",
    "    \n",
    "    # generate & align bootleg scores\n",
    "    nhdata = [(int(np.round(nhlocs[i][0])), int(np.round(nhlocs[i][1])), nhvals[i], nhclusters[i]) for i in range(len(nhlocs))]\n",
    "    bscore, events, eventIndices, staffLinesBoth = generateImageBootlegScore(nhdata, clusterPairs, bootlegRepeatNotes, bootlegFiller, minColDiff = nhwidth_est)\n",
    "        \n",
    "    # profile & save to file\n",
    "    profileEnd = time.time()\n",
    "    profileDur = profileEnd - profileStart\n",
    "    saveToFile(outfile, bscore, events, eventIndices, staffLinesBoth, bootlegFiller, estStaffLineLocs, \n",
    "               staveMidpts, staveMapping, targetLineSep, scale_factor, profileDur)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveEmptyResult(outfile, errorStr):\n",
    "    d = {'bscore': None, 'error': errorStr}\n",
    "    with open(outfile, 'wb') as f:\n",
    "        pickle.dump(d, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveToFile(outfile, bscore, events, eventIndices, stafflines, filler, estStaffLineLocs, staveMidpts, staveMapping, targetLineSep, scale_factor, dur):\n",
    "    d = {'bscore': bscore, 'events': events, 'eventIndices': eventIndices, 'stafflines': stafflines, 'filler': filler,\n",
    "         'estStaffLineLocs': estStaffLineLocs, 'staveMidpts': staveMidpts, 'staveMapping': staveMapping, 'targetLineSep': targetLineSep,\n",
    "         'scale_factor': scale_factor, 'dur': dur}\n",
    "    with open(outfile, 'wb') as f:\n",
    "        pickle.dump(d, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processAllImageFiles(pdflist, png_dir, outdir_root):\n",
    "    if not os.path.isdir(outdir_root):\n",
    "        os.makedirs(outdir_root)\n",
    "    with open(pdflist, 'r') as f:\n",
    "        for line in f:\n",
    "            pdffile = line.rstrip() # e.g. path/p135.pdf\n",
    "            pieceid = os.path.splitext(os.path.basename(pdffile))[0]\n",
    "            indir = '{}/{}'.format(png_dir, pieceid)\n",
    "            outdir = '{}/{}'.format(outdir_root, pieceid)\n",
    "            if not os.path.isdir(outdir):\n",
    "                os.makedirs(outdir)\n",
    "            for imagefile in glob.glob('{}/*.png'.format(indir)):\n",
    "                basename = os.path.splitext(os.path.basename(imagefile))[0] # e.g. path/p135-0.png\n",
    "                outfile = '{}/{}.pkl'.format(outdir, basename)\n",
    "                if os.path.exists(outfile):\n",
    "                    print('Skipping {}'.format(imagefile))\n",
    "                else:\n",
    "                    processImageFile(imagefile, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use single core\n",
    "# pdflist = 'cfg_files/db.list' # list of pdf scores\n",
    "# png_dir = 'data/png' # root directory containing image data\n",
    "# outdir = 'score_feat' # where to save bootleg scores\n",
    "# processAllImageFiles(pdflist, png_dir, outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use multiple cores\n",
    "pdflist = 'cfg_files/db.list' # list of pdf scores\n",
    "png_dir = 'data/png' # root directory containing image data\n",
    "feat_dir = 'score_feat' # where to save bootleg scores\n",
    "n_cores = 1 #multiprocessing.cpu_count()\n",
    "\n",
    "if not os.path.isdir(feat_dir):\n",
    "    os.makedirs(feat_dir)\n",
    "\n",
    "# prep inputs for parallelization\n",
    "inputs = []\n",
    "with open(pdflist, 'r') as f:\n",
    "    for line in f:\n",
    "        pdffile = line.rstrip() # e.g. path/p135.pdf\n",
    "        pieceid = os.path.splitext(os.path.basename(pdffile))[0] # e.g. p135\n",
    "        indir = '{}/{}'.format(png_dir, pieceid) # e.g. data/png/p135\n",
    "        outdir = '{}/{}'.format(feat_dir, pieceid) # e.g. score_feat/p135\n",
    "        if not os.path.isdir(outdir):\n",
    "            os.makedirs(outdir)\n",
    "        for imagefile in glob.glob('{}/*.png'.format(indir)):\n",
    "            basename = os.path.splitext(os.path.basename(imagefile))[0] # e.g. p135-0\n",
    "            outfile = '{}/{}.pkl'.format(outdir, basename)\n",
    "            if os.path.exists(outfile):\n",
    "                #print('Skipping {}'.format(os.path.basename(outfile)))\n",
    "                pass\n",
    "            else:\n",
    "                inputs.append((imagefile, outfile))\n",
    "print('{} remaining files to process'.format(len(inputs)))\n",
    "\n",
    "# process queries in parallel\n",
    "pool = multiprocessing.Pool(processes=n_cores)\n",
    "outputs = list(pool.starmap(processImageFile, inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SheetMidiRetrieval",
   "language": "python",
   "name": "sheetmidiretrieval"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
